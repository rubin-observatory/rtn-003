\section{Requirements and Guidelines for IDACs}\label{sec:reqs}
Since creating, delivering, and supporting the implementation of \RO data products via IDACs creates some cost to the \RO Project, IDACs will be expected to follow some basic requirements and guidelines, which are described below.
The actual costs of \gls{IDAC} support and infrastructure development are considered separately in Section \ref{sec:xfercost}.
We should also consider that there is the possibility of a {\it lite} DAC or a {\it full} DAC.


\subsection{\RO site topology} \label{sec:topology}

Figure \ref{fig:idac-topology} shows a  topology for a set of interconnected IDACs.  \gls{US} scientists will have direct access to the \RO \gls{US} Data Access Facility.  Hosting on the cloud is shown.

\begin{figure}
\begin{center}
\includegraphics[width=0.95\textwidth]{images/idac-topology}
\caption{US Data Facility and \gls{IDAC} network topology.  \label{fig:idac-topology}}
\end{center}
\end{figure}

\subsection{Authentication and Access}\label{sec:auth}
Any DAC will have to support authentication according to \RO access rules. This may imply delegating access control to a \RO authorization service. In addition any user with access rights should be allowed access to the IDAC.

We should be clear that on the construction side we have not necessarily planned to Authenticate users at various IDACs so there may be some development work needed to make an Auth service available.

\subsection{Required Resources} \label{sec:resources}
Institutions or organizations wishing to set up independent data access centres will be expected to have
sufficient resources and commitments before we discuss data transfers and support.
See also \secref{sec:cvs} for a discussion on compute vs storage.
The exact commitment of course depends on the level of IDAC being implemented.

\subsubsection{Data Storage}
Any institution considering setting up an \gls{IDAC} will need to show commitment on purchasing sufficient storage and \gls{CPU} power to hold and serve the data. Sufficient storage ranges from $0.5$ exabytes for the full data release(s) down to $100$ terabytes for a catalog server, and potentially further down to $70$ terabytes if the {\tt Object Lite} option is offered. For the full catalog , of order 100 nodes are required to serve it up. To serve images, a \gls{DAC} would need some additional servers; depending on load this may be order 10 additional nodes.

\subsubsection{Dedicated Personnel}
The significant hardware required by a full  \gls{IDAC} is above the normal level for most astronomy departments, and would require dedicated technical personnel to set it up and keep it running. For an {\tt Object Lite} catalog running on existing hardware, this might not be a significant increase in person power if the hardware is already serving on order $50$--$100$ terabytes. Still, it is recommended to assume $\gtrsim0.25$ full-time equivalent (\gls{FTE}) personnel hours for {\tt Object Lite}, and perhaps closer to $\sim2$ \gls{FTE} for the full catalogs, which includes setting up and maintaining the service, and installing new data releases and software updates every year. For IDACs wishing to host the full data releases' images and catalogs and deploy the \gls{RSP}, it becomes necessary to employ $1$--$2$ storage engineers to mange the large amount of data, and possibly one more \gls{FTE} to keep the \gls{Kubernetes} (or equivalent) system updated with the latest software deploys. If the \gls{IDAC} intends to support the science of many local users, support will become a specific issue which may not be covered by the usual institutional funding, and will require further effort. It is therefore recommended that any partner institution wishing to host a full-release \gls{IDAC} provide a minimum personnel of 5 \gls{FTE} to be considered viable.

\subsection{Networking and distribution}
There is an assumption than any prospective \gls{IDAC} will have a high bandwidth connection.
Any full \gls{IDAC} should  demonstrate sustained $20 \gls{Gb}/s$ to enable data transfer and sufficient bandwidth for access by users.
In addition all IDACs should be ready  to serve the {\tt \gls{Object} Lite} catalog to any institution worldwide but especially any {\em local} institutions. This should be thought of as a redistribution mechanism for the catalogs.

\subsection{Services for Full IDACs}

Full IDACs will be expected to provide services analogous to those provided at the \gls{US} Data Facility.

\subsubsection{The Rubin \gls{Science Platform}}

The {\it Rubin \gls{Science Platform}}  is a set of integrated web applications and services deployed at  \RO Data Access Centers through which the scientific community will access, visualize, subset and perform next-to-the-data analysis of the data collected by the \RO; it is envisioned to enable science cases that would greatly benefit from the co-location of user processing and \gls{LSST} data. It will provide users access to the {\tt Data Products} described in \ref{sec:data}, such as, resources for image reprocessing, access to the \RO processing framework, and many other services as described in \citeds{LSE-163}.  All full \gls{IDAC} will be expected to run and support the \gls{RSP}.

The \gls{RSP} is described in full in \citeds{LSE-319} and \citeds{LDM-554}. Depending on the assumed load, the \gls{RSP} is relatively modest as it requires only $\sim2$ servers to set up, and it is recommended to have 2 CPUs per simultaneous user (e.g., if the \gls{IDAC}'s desired capability is to serve 200 users, but only expect 50 to be active at a time, then 100 CPUs would be sufficient). From that starting point, the amount of next-to-the-data computational resources can be as large as the data center wishes to provide, and may make use of connecting to e.g., local super computer resources.
\subsubsection{User Generated data products }

{\it User Generated} data products will be created by the community deriving from the {\it Prompt} and {\it \gls{Data Release}} data products, and making  full use of the power of the \RO database systems and
Science Platform for the storage, access, and analysis of their results.
The \gls{Science Platform} will allow for the creation of {\it User Generated} data products and will enable science cases that greatly benefit from co-location of user processing and/or data within the USDF. Full IDACs will be expected to provide support for the creation of {\it User Generated} data products and their federation with the \gls{LSST} Data products.

\section{Responsibilities of the \gls{US} Data Facility}

This section describes the services that the USDF will provide in support of all  IDACs.

The \gls{LDF} will prepare data products for distribution to IDACs along with documentation of hardware and software that will make serving \RO data consistent with the serving of data from the USDF. \RO will provide (modest) technical support consistent with available resources to assist groups setting up IDACs.

LSST, through the USDF will establish a process for potential \gls{IDAC} groups to interface with and establish data transfers to their IDACs. It is expected that \gls{IDAC} groups will propose to \gls{LSST} what their \gls{IDAC} would support and then \gls{LSST} will work with them to establish requirements to receive \gls{LSST} data. One approved, \gls{LSST} will provide (modest) technical support consistent with available resources to assist groups setting up their IDACs provided they comply with prerequisites discussed in this document and especially in \secref{sec:resources}.

\subsection{Data Distribution} \label{sec:dist}

USDF will have $100Gb/s$ connections  on \gls{ESNet} which has interconnects with Internet2 - this should provide a distribution mechanism for getting data to IDACs, it will however be limited by the fact that much of our bandwidth is already allocated for data transmission to \gls{IN2P3} and alert distribution.

A tiered model as used by \gls{CERN} for high energy physics would seem a desirable way to achieve big transfers. Hence we would have a small selection of tier 2 centres with all data products from which tier 3 centres could copy the subsets they wish to work with.  Other alternatives are discuss in \secref{sec:xfer}.

In \gls{HEP} experiments such as  BaBar various physics analysis groups (science collaborations in \gls{LSST} ) were assigned to specific international centers as their primary computing and analysis facility, thereby distributing the computing load around the "network." Users naturally tend to use the facility with available resources and cycles.

As of 2021 the baseline for large transfers looks like Rucio\footnote{\url{https://rucio.cern.ch/}}.


