\section{Cost Impacts}\label{sec:costs}

As previously mentioned, standing up and maintaining multiple IDACs comes at a significant cost impact to both the \RO Project and the partner institutions. Minimizing these costs -- or at least maximizing the amount of science they enable -- should be at the forefront of all considerations concerning partner IDACs, such as the following propositions.

\subsection{Maximizing Profits with Science-Driven IDACs}
There are two main cost impacts of IDACs being set up outside of the \gls{US} and Chilean DACs: the positive impact is that some computational load may be taken off of these existing DACs, but the negative impact is the level of support required from the \RO Project in order to get them set up and running. This negative impact could be mitigated by ensuring that science productivity is maximized as a result of this extended effort. One way to do this might be to associate specific areas of science to a given \gls{IDAC}, and encourage users working in that field to use that \gls{IDAC}. This could create a customer base for the \gls{IDAC}, bring together like-minded experts, and effectively distribute the computing load across a network of IDACs. This might also enhance internal funding arguments for investment resources by arguing for synergies with local science goals and attracting international users and official endorsement.

\subsection{Data Transfer}\label{sec:xfer}
Even with good networks the data transfer will not be trivial, and could be quite expensive. \RO is not currently set up to distribute data to multiple sites, i.e., there is no form of peer-to-peer sharing. The bandwidth at \gls{USDF} is adequate for receiving data and delivering {\tt Alerts} to brokers during the night; perhaps some day time bandwidth could be used to transfer data to IDACs. A full data release of images and catalogs does not have to transferred within a given day; if the correct agreements are in place with an \gls{IDAC}, a full release could be transferred slowly as it is produced, and then made available to the IDACs users in whole on the official release day.
\input{xfercost}

\subsection{Compute {\it vs.} Storage Resources} \label{sec:cvs}
Data storage could be a large cost to IDACs, and could be considered as an overhead relative to the amount of computational resources an \gls{IDAC} can offer. If a full \gls{IDAC} is set up without a large compute capacity, the facility might be less useful to the science community than e.g., augmenting an existing DAC or \gls{IDAC} to have more computational resources. It is conceivable that a partner institution may prefer to spend their money increasing the computational quotas available for a given collaboration or set of PIs, and it would be scientifically beneficial if this was possible at all DAC and IDACs. The notion of standard compute quotas and resource allocation committees to adjudicate on large proposals for substantial increases to computational allocations are described in \citeds{LDO-13}. Another way to approach a solution to this issue might be to have a \emph{Cloud}-based \gls{IDAC} where a user or \gls{PI} could buy nodes on the provider cloud to access the holdings put there by \RO.  Such an option may be particularly useful to Science Collaborations with large compute needs.

The full sizing model is in \citeds{DMTN-135} - any \gls{IDAC} should have a similar sizing model. They may not need as much compute or as many copies of data as we have but the raw information to make such calls are in the technote.  For ball park figures the construction
to first year of ops table is copied here as \tabref{tab:opsSumUSDF}\footnote{ There is no guarantee of being in sync with \citeds{DMTN-135} but as an order of magnitude it is good.}.

\input{summary}
